#summary Improve OpenJPEG2000 Encoding Decoding Time


= Improve openjpeg2000 encoding/decoding time =

Jpeg2000 provides superior compression and advanced features like optionally lossless compression, region of interest coding, stream decoding etc. As a result of this complex feature set, the encoding and decoding process for jpeg2000 is computationally expensive. It has already been demonstrated that a significant speed up is achieved in many image processing applications by using the massively parallel GPU architecture. In fact there is previous literature that reports speed up on the GPUs for various components of jpeg2000 like DWT, EBCOT etc.

As a part of this project we plan to develop a parallel implementation for jpeg2000 encoding/decoding using the CUDA programming platform available for Nvidia GPUs. The decompression is more challenging than the compression and we plan to focus on the lossy decoding. At the end of this project, we hope to have a parallel implementation for tier 1/2 decoding, inverse dwt and inverse dc level shift which comprise the decoding pipeline.

= Code Repository =

The code for this project is pushed into [https://gitorious.org/~aditya12agd5/openjpeg-optimization/aditya12agd5s-openjpeg-optimization openjpeg optimization branch]

= Progress =

== Compilation ==

As a part of [https://gitorious.org/~aditya12agd5/openjpeg-optimization/aditya12agd5s-openjpeg-optimization/commit/922a0f3a8f626b19f27953917a19641faba150a8 commit], the appropriate changes were done to the cmake files to enable compilation of CUDA code with openjpeg library.
The file gpu.cu, contains all the CUDA kernels and also the kernel wrappers. The kernel wrappers can be evoked from the openjpeg library files and these wrappers then invoke the appropriate kernel. The kernel wrapper functions are prefixed with <b>gpu</b> and kernel functions are prefixed with <b>kernel</b>

== Inverse DC Level Shift ==

In [https://gitorious.org/~aditya12agd5/openjpeg-optimization/aditya12agd5s-openjpeg-optimization/commit/e73b93602d9af9d9bb9cd02bbe5241072fe5a6e5 commit], the function for inverse dc level shift was implemented on the GPU (gpu[ _ ]dc[ _ ]level[ _ ]shift[ _ ]decode). The data is copied component by component to the GPU.
The number of threads is equal to the image size and each thread adds the dc level shift value to the corresponding pixel. 

Once, the entire pipeline has been implemented, we can remove the memory transfer
overhead for this stage. Ideally before the first stage the image data is transferred to the GPU. It then continues to reside and also gets modified as the decoding stages t1/t2, inverse dwt and inverse dc level shift are performed. Finally after all stages, the output image is ready in the GPU memory and it is transferred back to the CPU output array.

== Inverse Discrete Wavelet Transform ==

The [https://gitorious.org/~aditya12agd5/openjpeg-optimization/aditya12agd5s-openjpeg-optimization/commit/84f53565c0a5578ad55dc95652a40c335f864e75 commit] has the version#1 of the complete implementation of the inverse DWT stage. The cuda implementation is as follows : 

1. Similar to the CPU code, processing of four values is done together using the float4 data type. Though a single cuda core does not have the vector computation capability (like MMX instructions on CPU), there is still benefit in using float4 because GPUs provide higher FLOPS/s and  and memory access is faster i.e. loading a float4 is quicker than individually loading the 4 floats.

2. For processing a rh x rw image in say decode[ _ ]h stage, the number of blocks is equal to rh/4 and each block has rw threads. A simpler way to understand is j^th thread of i^th block process four values : (4*i,j); (4*i+1,j); (4*i+2.j); (4*i+3;j). If rw is less than a threshold (currently 512), then the entire wavelet array of size rw per thread can be stored in the shared memory. Thus provided that the size of current resolution is less than a threshold we use the kernel with optimization of shared memory.

3. If the size exceeds the threshold, then we can no longer use shared memory and a global memory array is used for the wavelet. The kernels which handle this case of overflow of shared memory have the [ _ ]global[ _ ] in their function names.

4. Note that processing the entire wavelet array of size _rw_ in a single block gives us a chance to use the block synchronization primitive [ __ ]synchthreads. Thus we can club the v4dwt[ _ ]interleave[ _ ](h/v) and v4dwt[ _ ]decode[ _ ]step1 and v4dwt[ _ ]decode[ _ ]step2 in a single kernel. Suck kernel fusion as and when possible results in optimal performance of the code.

 
...more to come...